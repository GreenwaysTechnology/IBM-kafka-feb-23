			        Kafka
.....................................................................................
			   Event Driven Arch
.....................................................................................
Distributed Arch:
.................

1.Monolothic
    Single app build,test,deploy,maintain
2.Microservices
    Single domain is broken into multiple apps and start communicating each other.

Event driven programming model introduced in 1970s.

Graphical User interface operating systems introduced the concept of event driven programming.

Event is somthing which happened when something is done.

if you "type a character" -action in the keyboard,os sends a signal to hardware layer.
when ever a character is typed we have to react(respond), character need to be captured and send to the monitor.
.....................................................................................
				Program to Program Interaction
.....................................................................................

In object oriented programming models, object to object interaction.
Object communications
why Object need to need communicate?
  To share data.

Object communication patterns:
.............................

Objects talk each other via apis .

1.local api call - with in same runtime
2.Remote api call -(RPC)- IPC(inter process communication)
   objects may be in two different machines/within same machines(two different process)  - Request -response pattern / Request - Reply
  Networking
     =>Transport layer -  protocal layer -  tcp
     =>Data format
          =>Serialization and deserialation
     =>network layer
         =>ip address,port no 
Tight Coupling between or among objects(caller , callee)
MOM Is solution  
  Message oriented Middlewares - Pub- Sub Pattern
....................................................................................
			  Sync and Async == Blocking and Non blocking
...................................................................................
Non blocking IO is implemented based on event driven  programming model.

Have you seen any where mom implementation with out message brokers?
   AKKA -actor models
vertx,quarkus
.....................................................................................
				Histroy of Kafka : Why kafka was created
....................................................................................
Use case 1: How does one do an analysis of app metrics?


What is kafka?
  
Kafka is event streaming platform used to collect,process,store and integrate data at scale.
Kafka collects data in the form  of events.

What is event?
 An event is any type of action,incident, or change that is identified or recorded by software or applications.
  for eg in ecommerce app, a payment,web site link or order processing,inventory recording.

How information is stored?
  information is stored in database as table which represents the state of something
eg user - id,name,city.

Can we store events in database?
  We cant store all events in the database.

Now a days instead of thinking "things" first, people starts thinking events first.
instead of stroing things into db, we store events.

events also has some state like "things"

"event has some description of what happended with it, but the primary idea is that event is an indication in time that thing took place.

"You cant store every events that happened in the system into database"
.....................................................................................
 	How do you store every events that happended in the system?

Log is the best eg in traditional system we capture about methods calling,transactions,security

log is structured and the sequence of the events occured in the method calls.

eg:
09:38:14.114 [main] ERROR com.company.Main - Hello there!
09:38:14.119 [main] FATAL com.company.Main - Hello there!

Just logs in the logger system, why cant we store every activity of the system(events) into a file system called "log".

When we write event into log file, we write little bit of state,little bit of desc what happened.

log is just disk file
....................................................................................
			  What is apache kafka?

Kafka is a software system for managining these logs using a fairly standard,the same historical term called "topics".

Topics:
  The events which are captureed by system, and organized into kafka using the sturcture called "topic"
 Topic is eq to database table.
 Topic is logical structure which stores sequence of information.
 Topic is log of events.

Feature of logs:
 => logs are easy to understand
 => logs are append only...
 => logs are immutable
 => logs are highly durable
....................................................................................
	Kafka is distributed system to capture "data" in the form of 
events stored into log files later which can be furture processed to create useful reports.
             Kafka is just a "file Manager,Processor" Applications.

Core concepts of kafka

Kafka storage model - topics,partitioning,segments,offset,message(Record)

Producer---->writes record--Topic----|Partition--|segment--|offset
.....................................................................................
			Kafka Setup
.....................................................................................

Kafka Distribution:
..................
1.Apache Kafka
   It is open source version of kafka
2.Confluent kafka
   It is abstraction of apache kafka
  Confluent is so much more than Apache Kafka

Apache kafka vs Confluent Kafka 
https://www.confluent.io/apache-kafka-vs-confluent/

Platforms:
1.bare metal 
 kafka is distributed for all os

1.windows : may be good for basic use cases
2.linux  : recommended for advanced use cases
3.mac : recommended for advanced use cases.

2.VM env
  you can setup kafka on any industry standard vms -  oracle virtual box

3.Container based distributed - docker and kubernets
    it is highly recommend for dev and also even in productions.

Bare metal means  Linux is highly recommened:
............................................

Windows Sub System - Linux -  wsl
https://www.confluent.io/blog/set-up-and-run-kafka-on-windows-linux-wsl-2/?utm_medium=sem&utm_source=google&utm_campaign=ch.sem_br.nonbrand_tp.prs_tgt.dsa_mt.dsa_rgn.india_lng.eng_dv.all_con.blog&utm_term=&creative=&device=c&placement=&gclid=CjwKCAiAioifBhAXEiwApzCztroeBf99rY2AQ8kqOxzpA4vWwv_pe4qIQ682fYDA6hF9JJSJ6gN61hoCJQwQAvD_BwE.

After installing :

we need to look at folder structure

subugee@LAPTOP-R2TGGFDL:~/session/kafka_2.13-2.6.0$ ls -al
total 60
drwxr-xr-x 6 subugee subugee  4096 Jul 28  2020 .
drwxr-xr-x 3 subugee subugee  4096 Feb  7 16:37 ..
drwxr-xr-x 3 subugee subugee  4096 Jul 28  2020 bin
drwxr-xr-x 2 subugee subugee  4096 Jul 28  2020 config
drwxr-xr-x 2 subugee subugee  4096 Feb  7 16:37 libs
-rw-r--r-- 1 subugee subugee 29975 Jul 28  2020 LICENSE
-rw-r--r-- 1 subugee subugee   337 Jul 28  2020 NOTICE
drwxr-xr-x 2 subugee subugee  4096 Jul 28  2020 site-docs
.....................................................................................
			kafka is just  java program
Kafka is written in java language

subugee@LAPTOP-R2TGGFDL:~/session/kafka_2.13-2.6.0/libs$ ls -al
total 63964
drwxr-xr-x 2 subugee subugee     4096 Feb  7 16:37 .
drwxr-xr-x 6 subugee subugee     4096 Jul 28  2020 ..
-rw-r--r-- 1 subugee subugee    69409 Apr  3  2018 activation-1.1.1.jar
-rw-r--r-- 1 subugee subugee    14212 May  8  2019 aopalliance-repackaged-2.5.0.jar
-rw-r--r-- 1 subugee subugee    90347 May  1  2017 argparse4j-0.7.0.jar
-rw-r--r-- 1 subugee subugee    20437 Jan 29  2018 audience-annotations-0.5.0.jar
-rw-r--r-- 1 subugee subugee    53820 Jan 26  2019 commons-cli-1.4.jar
etc...

lib folder contains all kafka dependencies in the form of jar.
...................................................................................
bin folder
  contains all scripts file for running kafka servers
  constains subfolder called windows which contains bat files for running kafka in windows os.


config$ 
connect-console-sink.properties    connect-file-source.properties   consumer.properties  tools-log4j.properties
connect-console-source.properties  connect-log4j.properties         log4j.properties     trogdor.conf
connect-distributed.properties     connect-mirror-maker.properties  producer.properties  zookeeper.properties
connect-file-sink.properties       connect-standalone.properties    server.properties

config contains all kafka related settings.
.....................................................................................
				Core concepts
.....................................................................................

Broker:

Since kafka is java program which is deployed on jvm, kafka runs on the jvm process.
 which is called as "Kafka Server"
It is composed of network of machines called brokers.

By default Kafka broker is distributed (Scalable-running multiple instance of same broker)

Cluster:
  A kafka cluster(cluster) is a system that consists of serveral brokers.
 The cluster helps to distribute workloads equally among replicas.

By default Kafka is clustered (distributed) commit log system.

Every distributed system need to talk each other, some body need to manage cluster.

in order to manage cluster we have cluster management softwares.

ZooKeeper:
..........

What is ZooKeeper?
	ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services.

Without Zookeeper we cant setup kafka cluster.

KRaft:
    Apache Kafka Raft (KRaft) is the consensus protocol that was introduced to remove Apache Kafka's dependency on ZooKeeper for metadata management.


Zookeeper role:
=>Cluster management
=>Failure detection and recover
=>Store ACL & Secrets

Lab 1: Kafka Setup Cluster
Single Broker,Single Zookeeper

Broker and zookeeper having configurations that has to be loaded as part of server startup.
configurations are kept inside config folder.

Steps:
1.Start Zookeeper

./bin/zookeeper-server-start.sh config/zookeeper.properties

zookeeper.properties:

dataDir=/tmp/zookeeper
  where zookeeper configuration data is kept during runtime.

# the port at which the clients will connect
clientPort=2181

# Disable the adminserver by default to avoid port conflicts.
# Set the port to something non-conflicting if choosing to enable this
admin.enableServer=false
# admin.serverPort=8080

2.Start Kafka Server

./bin/kafka-server-start.sh config/server.properties

server.properties

log.dirs=/tmp/kafkalogs
.....................................................................................
How to start kafka and zookeeper server in background?

./bin/zookeeper-server-start.sh -daemon config/zookeeper.properties

./bin/kafka-server-start.sh -daemon config/server.properties

Test servers are running:
 telnet  localhost 2181
 telnet  localhost 9092
.....................................................................................
			Overall Archiecture of Kafka

Kafka is a data streaming system that allows developers to react to new events as they occur in real time. Kafka architecture consists of a storage layer and a compute layer. The storage layer is designed to store data efficiently and is a distributed system such that if your storage needs grow over time you can easily scale out the system to accommodate the growth. The compute layer consists of four core components—the producer, consumer, streams, and connector APIs, which allow Kafka to scale applications across distributed systems. 

Producer and Consumer APIs
The foundation of Kafka’s powerful application layer is two primitive APIs for accessing the storage—the producer API for writing events and the consumer API for reading them. On top of these are APIs built for integration and processing.

Kafka Connect
Kafka Connect, which is built on top of the producer and consumer APIs, provides a simple way to integrate data across Kafka and external systems. Source connectors bring data from external systems and produce it to Kafka topics. Sink connectors take data from Kafka topics and write it to external systems.

Kafka Streams
For processing events as they arrive, we have Kafka Streams, a Java library that is built on top of the producer and consumer APIs. Kafka Streams allows you to perform real-time stream processing, powerful transformations, and aggregations of event data.

ksqlDB
Building on the foundation of Kafka Streams, we also have ksqlDB, a streaming database which allows for similar processing but with a declarative SQL-like syntax.
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,


Event
An event is a record of something that happened that also provides information about what happened. Examples of events are customer orders, payments, clicks on a website, or sensor readings. An event shouldn’t be too large. A 10GB video is not a good event. A reference to the location of that video in an object store is.

An event record consists of a timestamp, a key, a value, and optional headers. The event payload is usually stored in the value. The key is also optional, but very helpful for event ordering, colocating events across topics, and key-based storage or compaction.

Schema registry:

In Kafka, the key and value are stored as byte arrays which means that clients can work with any type of data that can be serialized to bytes.

A popular format among Kafka users is Avro, which is also supported by Confluent Schema Registry.

When integrated with Schema Registry, the first byte of an event will be a magic byte which signifies that this event is using a schema in the Schema Registry. The next four bytes make up the schema ID that can be used to retrieve the schema from the registry, and the rest of the bytes contain the event itself. Schema Registry also supports Protobuf and JSON schema formats.
....................................................................................
				Topic
....................................................................................

What is Topic?
  Topic is logical structure of storing events(records,messages) into kafka broker(cluster) which is eq to table.

How to create Topic?
 Kafka cli provides a script called "kafka-topics.sh"

Using this script
=>create topic
=>list topics
=>describe topic
=>delete topic


Get Help 
./bin/kafka-topics.sh

Create, delete, describe, or change a topic.

Option                                   Description
------                                   -----------
--alter                                  Alter the number of partitions,
                                           replica assignment, and/or
                                           configuration for the topic.
--at-min-isr-partitions                  if set when describing topics, only
                                           show partitions whose isr count is
                                           equal to the configured minimum. Not
                                           supported with the --zookeeper
                                           option.
--bootstrap-server <String: server to    REQUIRED: The Kafka server to connect
  connect to>                              to. In case of providing this, a
                                           direct Zookeeper connection won't be
                                           required.
--command-config <String: command        Property file containing configs to be
  config property file>                    passed to Admin Client. This is used
                                           only with --bootstrap-server option
                                           for describing and altering broker
                                           configs.
--config <String: name=value>            A topic configuration override for the
                                           topic being created or altered.The
                                           following is a list of valid
                                           configurations:
                                                cleanup.policy
                                                compression.type
                                                delete.retention.ms
                                                file.delete.delay.ms
                                                flush.messages
                                                flush.ms
                                                follower.replication.throttled.
                                           replicas
                                                index.interval.bytes
                                                leader.replication.throttled.replicas
                                                max.compaction.lag.ms
                                                max.message.bytes
                                                message.downconversion.enable
                                                message.format.version
                                                message.timestamp.difference.max.ms
                                                message.timestamp.type
                                                min.cleanable.dirty.ratio
                                                min.compaction.lag.ms
                                                min.insync.replicas
                                                preallocate
                                                retention.bytes
                                                retention.ms
                                                segment.bytes
                                                segment.index.bytes
                                                segment.jitter.ms
                                                segment.ms
                                                unclean.leader.election.enable
                                         See the Kafka documentation for full
                                           details on the topic configs.It is
                                           supported only in combination with --
                                           create if --bootstrap-server option
                                           is used.
--create                                 Create a new topic.
--delete                                 Delete a topic
--delete-config <String: name>           A topic configuration override to be
                                           removed for an existing topic (see
                                           the list of configurations under the
                                           --config option). Not supported with
                                           the --bootstrap-server option.
--describe                               List details for the given topics.
--disable-rack-aware                     Disable rack aware replica assignment
--exclude-internal                       exclude internal topics when running
                                           list or describe command. The
                                           internal topics will be listed by
                                           default
--force                                  Suppress console prompts
--help                                   Print usage information.
--if-exists                              if set when altering or deleting or
                                           describing topics, the action will
                                           only execute if the topic exists.
--if-not-exists                          if set when creating topics, the
                                           action will only execute if the
                                           topic does not already exist.
--list                                   List all available topics.
--partitions <Integer: # of partitions>  The number of partitions for the topic
                                           being created or altered (WARNING:
                                           If partitions are increased for a
                                           topic that has a key, the partition
                                           logic or ordering of the messages
                                           will be affected). If not supplied
                                           for create, defaults to the cluster
                                           default.
--replica-assignment <String:            A list of manual partition-to-broker
  broker_id_for_part1_replica1 :           assignments for the topic being
  broker_id_for_part1_replica2 ,           created or altered.
  broker_id_for_part2_replica1 :
  broker_id_for_part2_replica2 , ...>
--replication-factor <Integer:           The replication factor for each
  replication factor>                      partition in the topic being
                                           created. If not supplied, defaults
                                           to the cluster default.
--topic <String: topic>                  The topic to create, alter, describe
                                           or delete. It also accepts a regular
                                           expression, except for --create
                                           option. Put topic name in double
                                           quotes and use the '\' prefix to
                                           escape regular expression symbols; e.
                                           g. "test\.topic".
--topics-with-overrides                  if set when describing topics, only
                                           show topics that have overridden
                                           configs
--unavailable-partitions                 if set when describing topics, only
                                           show partitions whose leader is not
                                           available
--under-min-isr-partitions               if set when describing topics, only
                                           show partitions whose isr count is
                                           less than the configured minimum.
                                           Not supported with the --zookeeper
                                           option.
--under-replicated-partitions            if set when describing topics, only
                                           show under replicated partitions
--version                                Display Kafka version.
--zookeeper <String: hosts>              DEPRECATED, The connection string for
                                           the zookeeper connection in the form
                                           host:port. Multiple hosts can be
                                           given to allow fail-over.



subugee@LAPTOP-R2TGGFDL:~/session/kafka_2.13-2.6.0$ ./bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic demo-topic
Created topic demo-topic.

How topic is represented inside disk?

 \tmp\kafka-logs
         |
         demo-topic-0

Topic is nothing but folder 
   here our topic name was demo-topic but what is "0" ? This is broker id
.....................................................................................
			     partition
.....................................................................................
Partitions:
  In order to distribute the "storage and processing of events" in a topic, Kafka uses the concept of partitions.

 A topic is made up of one or more partitions and these partitions can reside on different nodes in the Kafka cluster.

The partition is the main unit of storage for Kafka events, although with Tiered Storage, which we’ll talk about later, some event storage is moved off of partitions. The partition is also the main unit of parallelism. Events can be produced to a topic in parallel by writing to multiple partitions at the same time. Likewise, consumers can spread their workload by individual consumer instances reading from different partitions. If we only used one partition, we could only effectively use one consumer instance.


How to describe how many partitions are there for a topic?

./bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic demo-topic
Topic: demo-topic  
  
PartitionCount: 1   ReplicationFactor: 1    Configs: segment.bytes=1073741824
Topic: demo-topic    Partition: 0    Leader: 0       Replicas: 0     Isr: 0

How to distribute messages into different partitions?

 ./bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic ibm-topic --partitions 3
Created topic ibm-topic.

After creating lets look at /tmp/kafka-logs folder

/tmp/kafka-logs
ibm-topic-0  => 0 partition number
ibm-topic-1
ibm-topic-2

./bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic ibm-topic

./bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic ibm-topic
 Topic: ibm-topic  

PartitionCount: 3       ReplicationFactor: 1    Configs: segment.bytes=1073741824
Topic: ibm-topic        Partition: 0    Leader: 0       Replicas: 0     Isr: 0
Topic: ibm-topic        Partition: 1    Leader: 0       Replicas: 0     Isr: 0
Topic: ibm-topic        Partition: 2    Leader: 0       Replicas: 0     Isr: 0
.................................................................................
				  Segment
..................................................................................

Topic is broken into Partition, Partition is broken into Segment.

What is segment is nothing but a single log file.
Each segment is stored in a single data file on the disk attached to the broker.

How to list segments(log files)?

/tmp/kafka-logs/ibm-topic-0$ ls -l
total 4
-rw-r--r-- 1 subugee subugee 10485760 Feb  8 16:37 00000000000000000000.index
-rw-r--r-- 1 subugee subugee        0 Feb  8 16:37 00000000000000000000.log
-rw-r--r-- 1 subugee subugee 10485756 Feb  8 16:37 00000000000000000000.timeindex
-rw-r--r-- 1 subugee subugee        8 Feb  8 16:37 leader-epoch-checkpoint

Here Segment is group of files
 .log
 .index
 .timeindex
  leader-epoch-chechkpoint
  partition.metadata

Segment is just log file where real data(Record/Message) is storied in the form of byte array.

Points:
 =>Once the topic is created initally only one segment file would be there.
 =>We can have multiple segment file, based on two properties configured 
     # The maximum size of a log segment file. When this size is reached a new log         segment will be created.
   log.segment.bytes=1073741824
   log.segment.ms|days|hours=5000000
      The kafka stores data for next 5 days, after that it creates new segment file
...................................................................................
		   How data(record/message is written into segment)
..................................................................................

log file contains two parts

offset : it is  sequence of number added into the file for referencing
payload : bytes

index file:
 Contains the mappings of offset to its position in ".log" file

timeindex file:
  File contains the mappings of timestamp to message offset
  Used to search messages based on timestamp.

How to verify the log details(message logs) ?

./bin/kafka-run-class.sh kafka.tools.DumpLogSegments --deep-iteration --print-data-log --files /tmp/kafka-logs/ibm-topic-0/00000000000000000000.log
Dumping /tmp/kafka-logs/ibm-topic-0/00000000000000000000.log
Starting offset: 0
..................................................................................

How to publish record(message/event)?

./bin/kafka-console-producer.sh

Missing required option(s) [bootstrap-server]

Option                                   Description
------                                   -----------
--batch-size <Integer: size>             Number of messages to send in a single
                                           batch if they are not being sent
                                           synchronously. (default: 200)
--bootstrap-server <String: server to    REQUIRED unless --broker-list
  connect to>                              (deprecated) is specified. The server
                                           (s) to connect to. The broker list
                                           string in the form HOST1:PORT1,HOST2:
                                           PORT2.
--broker-list <String: broker-list>      DEPRECATED, use --bootstrap-server
                                           instead; ignored if --bootstrap-
                                           server is specified.  The broker
                                           list string in the form HOST1:PORT1,
                                           HOST2:PORT2.
--compression-codec [String:             The compression codec: either 'none',
  compression-codec]                       'gzip', 'snappy', 'lz4', or 'zstd'.
                                           If specified without value, then it
                                           defaults to 'gzip'
--help                                   Print usage information.
--line-reader <String: reader_class>     The class name of the class to use for
                                           reading lines from standard in. By
                                           default each line is read as a
                                           separate message. (default: kafka.
                                           tools.
                                           ConsoleProducer$LineMessageReader)
--max-block-ms <Long: max block on       The max time that the producer will
  send>                                    block for during a send request
                                           (default: 60000)
--max-memory-bytes <Long: total memory   The total memory used by the producer
  in bytes>                                to buffer records waiting to be sent
                                           to the server. (default: 33554432)
--max-partition-memory-bytes <Long:      The buffer size allocated for a
  memory in bytes per partition>           partition. When records are received
                                           which are smaller than this size the
                                           producer will attempt to
                                           optimistically group them together
                                           until this size is reached.
                                           (default: 16384)
--message-send-max-retries <Integer>     Brokers can fail receiving the message
                                           for multiple reasons, and being
                                           unavailable transiently is just one
                                           of them. This property specifies the
                                           number of retires before the
                                           producer give up and drop this
                                           message. (default: 3)
--metadata-expiry-ms <Long: metadata     The period of time in milliseconds
  expiration interval>                     after which we force a refresh of
                                           metadata even if we haven't seen any
                                           leadership changes. (default: 300000)
--producer-property <String:             A mechanism to pass user-defined
  producer_prop>                           properties in the form key=value to
                                           the producer.
--producer.config <String: config file>  Producer config properties file. Note
                                           that [producer-property] takes
                                           precedence over this config.
--property <String: prop>                A mechanism to pass user-defined
                                           properties in the form key=value to
                                           the message reader. This allows
                                           custom configuration for a user-
                                           defined message reader. Default
                                           properties include:
                                                parse.key=true|false
                                                key.separator=<key.separator>
                                                ignore.error=true|false
--request-required-acks <String:         The required acks of the producer
  request required acks>                   requests (default: 1)
--request-timeout-ms <Integer: request   The ack timeout of the producer
  timeout ms>                              requests. Value must be non-negative
                                           and non-zero (default: 1500)
--retry-backoff-ms <Integer>             Before each retry, the producer
                                           refreshes the metadata of relevant
                                           topics. Since leader election takes
                                           a bit of time, this property
                                           specifies the amount of time that
                                           the producer waits before refreshing
                                           the metadata. (default: 100)
--socket-buffer-size <Integer: size>     The size of the tcp RECV size.
                                           (default: 102400)
--sync                                   If set message send requests to the
                                           brokers are synchronously, one at a
                                           time as they arrive.
--timeout <Integer: timeout_ms>          If set and the producer is running in
                                           asynchronous mode, this gives the
                                           maximum amount of time a message
                                           will queue awaiting sufficient batch
                                           size. The value is given in ms.
                                           (default: 1000)
--topic <String: topic>                  REQUIRED: The topic id to produce
                                           messages to.
--version                                Display Kafka version.


Note:
We are going to publish message into topic only

producer--->record-->publish---Topic|---partitions--|each partition has its own log file.

./bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic ibm-topic
>Hello IBM
press ctrl key

subugee@LAPTOP-R2TGGFDL:~/session/kafka_2.13-2.6.0$ ./bin/kafka-run-class.sh kafka.tools.DumpLogSegments --deep-iteration --print-data-log --files /tmp/kafka-logs/ibm-topic-0/00000000000000000000.log
Dumping /tmp/kafka-logs/ibm-topic-0/00000000000000000000.log
Starting offset: 0

subugee@LAPTOP-R2TGGFDL:~/session/kafka_2.13-2.6.0$ ./bin/kafka-run-class.sh kafka.tools.DumpLogSegments --deep-iteration --print-data-log --files /tmp/kafka-logs/ibm-topic-1/00000000000000000000.log

Dumping /tmp/kafka-logs/ibm-topic-1/00000000000000000000.log

Starting offset: 0
baseOffset: 0 lastOffset: 0 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: 0 CreateTime: 1675933782330 size: 77 magic: 2 compresscodec: NONE crc: 2826934114 isvalid: true
| offset: 0 CreateTime: 1675933782330 keysize: -1 valuesize: 9 sequence: -1 headerKeys: [] payload: Hello IBM


subugee@LAPTOP-R2TGGFDL:~/session/kafka_2.13-2.6.0$ ./bin/kafka-run-class.sh kafka.tools.DumpLogSegments --deep-iteration --print-data-log --files /tmp/kafka-logs/ibm-topic-2/00000000000000000000.log

Dumping /tmp/kafka-logs/ibm-topic-2/00000000000000000000.log
Starting offset: 0
.....................................................................................
			How producers publish messages?
....................................................................................


Publish Message:

Producer Default Partitioner & Sticky Partitioner:

	A partitioner is the process that will determine to which partition a specific message will be assigned to.
       In nutshell Partitioner is simple java class / program , having routing algorthim (which partition to be selected) for eg i have 3 partition, which partition i have to send "this" message.

How partitions is selected by Partitioner?
 =>Based on Record only
      
Partitioner when key!=null, Having Key
.......................................

 if record is having key,

   Key Hashing is the process of determining the mapping of a key to a partition In the default Kafka partitioner, the keys are hashed using the murmur2 algorithm

targetPartition = Math.abs(Utils.murmur2(keyBytes)) % (numPartitions - 1)

This means that same key will go to the same partition and adding partitions to a topic will completely alter the formula. It is most likely preferred to not override the behavior of the partitioner, but it is possible to do so using partitioner.class.

Partitioner when key=null
.........................

When key=null, the producer has a default partitioner that varies:

Round Robin: for Kafka 2.3 and below

Sticky Partitioner: for Kafka 2.4 and above

Sticky Partitioner improves the performance of the producer especially with high throughput.


RoundRobin:
With Kafka ≤ v2.3, when there’s no partition and no key specified, the default partitioner sends data in a round-robin fashion. This results in more batches (one batch per partition) and smaller batches (imagine with 100 partitions). And this is a problem because smaller batches lead to more requests as well as higher latency.

Sticky Partitioner (Kafka ≥ 2.4)

It is a performance goal to have all the records sent to a single partition and not multiple partitions to improve batching.

The producer sticky partitioner will:

“stick” to a partition until the batch is full or linger.ms has elapsed

After sending the batch, change the partition that is "sticky"

This will lead to larger batches and reduced latency (because we have larger requests, and the batch.size is more likely to be reached). Over time, the records are still spread evenly across partitions, so the balance of the cluster is not affected.

Overall, there are some substantial performance improvements by using the Sticky Partitioner.

Stick Patitioner = Batching + Round Robin
Lab:
Publish Message without Key
./bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic ibm-topic
>Hello
Press ctrl key
./bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic ibm-topic
>How are you

Press ctrl key
./bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic ibm-topic
>Welcome
....................................................................................
				Publish Record with Keys
.....................................................................................

By default publishers send records without key called key is null

if you want to send Record with key there are additional properties need to passed/configured as part of cli.

--property <String: prop>                A mechanism to pass user-defined
                                           properties in the form key=value to
                                           the message reader. This allows
                                           custom configuration for a user-
                                           defined message reader. Default
                                           properties include:
                                                parse.key=true|false
                                                key.separator=<key.separator>
                                                ignore.error=true|false

./bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic ibm-topic --property "key.separator=:" --property "parse.key=true"
...................................................................................
				Consumer
...................................................................................
How to consume records?

=> Find kafka host name and port no eg : localhost:9092
=> Provide topic name eg  ibm-topic
=> if you want to read future message(current message published) 
=> if you want to read histrical message, --from-beginning option
=> we can  use kafka-console-consumer.sh 


 ./bin/kafka-console-consumer.sh
This tool helps to read data from Kafka topics and outputs it to standard output.
Option                                   Description
------                                   -----------
--bootstrap-server <String: server to    REQUIRED: The server(s) to connect to.
  connect to>
--consumer-property <String:             A mechanism to pass user-defined
  consumer_prop>                           properties in the form key=value to
                                           the consumer.
--consumer.config <String: config file>  Consumer config properties file. Note
                                           that [consumer-property] takes
                                           precedence over this config.
--enable-systest-events                  Log lifecycle events of the consumer
                                           in addition to logging consumed
                                           messages. (This is specific for
                                           system tests.)
--formatter <String: class>              The name of a class to use for
                                           formatting kafka messages for
                                           display. (default: kafka.tools.
                                           DefaultMessageFormatter)
--from-beginning                         If the consumer does not already have
                                           an established offset to consume
                                           from, start with the earliest
                                           message present in the log rather
                                           than the latest message.
--group <String: consumer group id>      The consumer group id of the consumer.
--help                                   Print usage information.
--isolation-level <String>               Set to read_committed in order to
                                           filter out transactional messages
                                           which are not committed. Set to
                                           read_uncommitted to read all
                                           messages. (default: read_uncommitted)
--key-deserializer <String:
  deserializer for key>
--max-messages <Integer: num_messages>   The maximum number of messages to
                                           consume before exiting. If not set,
                                           consumption is continual.
--offset <String: consume offset>        The offset id to consume from (a non-
                                           negative number), or 'earliest'
                                           which means from beginning, or
                                           'latest' which means from end
                                           (default: latest)
--partition <Integer: partition>         The partition to consume from.
                                           Consumption starts from the end of
                                           the partition unless '--offset' is
                                           specified.
--property <String: prop>                The properties to initialize the
                                           message formatter. Default
                                           properties include:
                                                print.timestamp=true|false
                                                print.key=true|false
                                                print.value=true|false
                                                key.separator=<key.separator>
                                                line.separator=<line.separator>
                                                key.deserializer=<key.deserializer>
                                                value.deserializer=<value.
                                           deserializer>
                                         Users can also pass in customized
                                           properties for their formatter; more
                                           specifically, users can pass in
                                           properties keyed with 'key.
                                           deserializer.' and 'value.
                                           deserializer.' prefixes to configure
                                           their deserializers.
--skip-message-on-error                  If there is an error when processing a
                                           message, skip it instead of halt.
--timeout-ms <Integer: timeout_ms>       If specified, exit if no message is
                                           available for consumption for the
                                           specified interval.
--topic <String: topic>                  The topic id to consume on.
--value-deserializer <String:
  deserializer for values>
--version                                Display Kafka version.
--whitelist <String: whitelist>          Regular expression specifying
                                           whitelist of topics to include for
                                           consumption.

eg:
./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic ibm-topic --from-beginning
Hello IBM
Hai IBM
1
2
A
B
c
d
Welcome to IBM
.....................................................................................

How to print records with extra properties?

Time Stamp:
 ./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic ibm-topic --from-beginning --property print.timestamp=true
CreateTime:1675933782330        Hello IBM
CreateTime:1675934204015        Hai IBM
CreateTime:1675936991017        1
CreateTime:1675936994936        2
CreateTime:1675936998681        A
CreateTime:1675937002418        B
CreateTime:1675937006054        c
CreateTime:1675937018086        d
CreateTime:1675939290483        kafka is good for event streaming
CreateTime:1675939291102
CreateTime:1675934174429        Welcome to IBM
CreateTime:1675939322905        kafka uses partition to distribute topic across the nodes

How to print records with key,value:

./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic ibm-topic --from-beginning --property print.timestamp=true --property print.key=true --property print.value=true
CreateTime:1675933782330        null    Hello IBM
CreateTime:1675934204015        null    Hai IBM
CreateTime:1675936991017        id      1
CreateTime:1675936994936        id      2
CreateTime:1675936998681        name    A
CreateTime:1675937002418        name    B
CreateTime:1675937006054        city    c
CreateTime:1675937018086        city    d
CreateTime:1675939290483        null    kafka is good for event streaming
CreateTime:1675939291102        null
CreateTime:1675934174429        null    Welcome to IBM
CreateTime:1675939322905        null    kafka uses partition to distribute topic across the nodes
....................................................................................
How to list partition on which the message is published?

--print.partition=true

Produce data with key and with out key and watch which partition is selected
............................................................................

./bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic ibm-topic --property "key.separator=:" --property "parse.key=true"
>project:kafka
>project:java
>project:microservice
>client:google
>client:ibm
>client:microsoft
>client:facebook
>payment:gpay
>payment:neft
>payment:rtgs
>transport:bus
>trapnsport:car
>skill:java
>id:2
>

Consumer:
 ./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic ibm-topic --from-beginning  --property print.key=true --property print.value=true --property print.partition=true
salary  2333    2
null    test    2
null    Hello IBM       1
null    Hai IBM 1
id      1       1
id      2       1
name    A       1
name    B       1
city    c       1
city    d       1
null    kafka is good for event streaming       1
null            1
null    Welcome to IBM  0
null    kafka uses partition to distribute topic across the nodes       0
null    java    0
null    100     2
null    200     0
null    300     2
null    500     1
null    800     0
project kafka   0
project java    0
project microservice    0
client  google  1
client  ibm     1
client  microsoft       1
client  facebook        1
payment gpay    2
payment neft    2
payment rtgs    2
transport       bus     2
trapnsport      car     2
skill   java    2
id      2       1

.....................................................................................

How to print offset infromation?

--property print.offset

....................................................................................
			     Consumer Groups

Consumers that are part of the same application and therefore perform the same logical job can grouped togther as a kafka consumer group.

A topic consits of many partitions.

Partions are are unit of paralleism for kafka consumers.

Why consumer group?
   
 The consumers in the group who can balance of reading data from the partitions
 Parallely

Kafka Consumer Group Id:
................
  in order to indicate to kafka consumers that they are part of the same specfic group, we must specify the consumer side setting group.id

Setting up Consumer Group:

1.you must have topic with min of two partitions
2.when you run consumer , need to assign group with --group option
3.open two or many consumer terminals based on your need.
4.you have to publish data to topic where you can see how consumers sharing the reads

 ./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic ibm-topic --group ibmgroup --property print.partition=true
90000   1
40000   0
23333   1
hello   0
.....................................................................................
				Producer Reslience
....................................................................................


 



















